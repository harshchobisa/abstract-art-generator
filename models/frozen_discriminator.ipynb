{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "128x128 abstract_art_gan_dis_frozen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCH9aDcbvsu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bfb152-e9e3-480a-ce6d-fc53002231d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "RCH9aDcbvsu7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgWYa21sfDbd"
      },
      "source": [
        "!wget \"https://storage.googleapis.com/kaggle-data-sets/742030/1936055/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210307%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210307T021813Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=15207efdc677d5c9de6e28516cc2f0808e0b38fc824245d1363fb34ca7f13aaa661c913409bcd6fc2a5c40485f4b09d0302d4b8f6acac652dcfafb068ec043e5c42b359bc8601df52ed99574ebd0b0edbb882723dc484a2c17f73160cd3143b071cee5f090d7df9b6825eb141e03b020005065ba90f077aee978a4933f40abec0255b7fdc7784415cfd882841af11e3970929c9689ed29ee2dd1563125bf1dc544a849a3c4f2d42b3728e52910891dbe479ecdd563164e79a2ee1f25a1b969265cd3be5469e13c950709711164388689852fafa3e8ca4d6cbeddcd1aa06adf1a7ef11a62ad24e29f6b3c4b5d65e0178de1538ab7fec235f52f90a2d375931e46\""
      ],
      "id": "lgWYa21sfDbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wSFurP4fQTK"
      },
      "source": [
        "mv archive* data.zip "
      ],
      "id": "-wSFurP4fQTK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEnxzNNUfi_4"
      },
      "source": [
        "!mkdir data_augmented\n",
        "!mkdir drive/MyDrive/gan_images_frozen_gen"
      ],
      "id": "DEnxzNNUfi_4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nORHfP-rzs4"
      },
      "source": [
        "ls"
      ],
      "id": "7nORHfP-rzs4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSN0PSZufUdj"
      },
      "source": [
        "!unzip data.zip"
      ],
      "id": "fSN0PSZufUdj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7IfMmLlhnSt"
      },
      "source": [
        "!pip3 install torchvision\n",
        "!pip3 install torchsummary"
      ],
      "id": "t7IfMmLlhnSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHgBkNMLfhl0"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "names = ['rotated', 'vertical_flip', 'horizontal_flip']\n",
        "functions = [Image.ROTATE_90, Image.FLIP_TOP_BOTTOM, Image.FLIP_LEFT_RIGHT]\n",
        "count = 0\n",
        "for i in range(2782):\n",
        "    if i % 278 == 0:\n",
        "        print(str((i*100)//2780) + \" percent done\")\n",
        "\n",
        "    im  = Image.open(f\"./Abstract_gallery/Abstract_image_{i}.jpg\")\n",
        "    im.save(f\"./data_augmented/Abstract_image_{count}.jpg\")\n",
        "    count += 1\n",
        "\n",
        "    for j in range(3):\n",
        "        new_image = im.transpose(functions[j])\n",
        "        new_image.save(f\"./data_augmented/Abstract_image_{count}.jpg\")\n",
        "        count += 1\n",
        "        \n",
        "print(\"finished\")"
      ],
      "id": "nHgBkNMLfhl0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy4w51UAWRln"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.tensorboard\n",
        "from torchvision import transforms\n",
        "import torchvision.utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n"
      ],
      "id": "dy4w51UAWRln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK34ZCa3WRls"
      },
      "source": [
        "# zero_channel = 0\n",
        "# one_channel = 0\n",
        "# two_channel = 0\n",
        "\n",
        "\n",
        "def normalize_and_resize_image(image_path):\n",
        "    preprocess = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    input_tensor = preprocess(image)\n",
        "\n",
        "    image = input_tensor\n",
        "\n",
        "    # zero_channel = (torch.std(image[0]), torch.mean(image[0]))\n",
        "    # one_channel = (torch.std(image[1]), torch.mean(image[1]))\n",
        "    # two_channel = (torch.std(image[2]), torch.mean(image[2]))\n",
        "\n",
        "\n",
        "    # image[0] = image[0] * torch.std(image[0]) + torch.mean(image[0])\n",
        "    # image[1] = image[1] * torch.std(image[1]) + torch.mean(image[1])\n",
        "    # image[2] = image[2] * torch.std(image[2]) + torch.mean(image[2])\n",
        "    \n",
        "    return image"
      ],
      "id": "IK34ZCa3WRls",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXLP9qphX8SK"
      },
      "source": [
        "def img_denorm(image, transpose=True):\n",
        "  mean = np.asarray([ 0.5, 0.5, 0.5 ])\n",
        "  std = np.asarray([ 0.5, 0.5, 0.5 ])\n",
        "\n",
        "  denormalize = transforms.Normalize((-1 * mean / std), (1.0 / std))\n",
        "  res = image.squeeze()\n",
        "\n",
        "  image = denormalize(res)\n",
        "\n",
        "  if transpose:\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "    \n",
        "  return(image)"
      ],
      "id": "TXLP9qphX8SK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PljgOvFSWRlt"
      },
      "source": [
        "class ArtDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, num_images=2782*4):\n",
        "        self.size = num_images\n",
        "            \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # data_path = \"./Abstract_gallery/\"\n",
        "        data_path = \"./data_augmented/\"\n",
        "        image_path = f\"Abstract_image_{index}.jpg\"\n",
        "        return (normalize_and_resize_image(data_path + image_path), 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n"
      ],
      "id": "PljgOvFSWRlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcAEwzeWRlt"
      },
      "source": [
        "train_dataset = ArtDataset()\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=128, shuffle=True\n",
        "    )"
      ],
      "id": "pkcAEwzeWRlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFPaaOVWRlu"
      },
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, layer_sizes=[256, 128, 2], dropout_prob=None, device=None):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        if dropout_prob is not None and dropout_prob > 0.5:\n",
        "            print(\"Are you sure dropout_prob is supposed to be greater than 0.5?\")\n",
        "            \n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 4, stride = 2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.leakyRelu = nn.LeakyReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 4, stride = 2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 4, stride = 2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 4, stride = 2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 4, stride = 2, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels = 1024, out_channels = 1, kernel_size = 4, stride = 1, padding=0)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.leakyRelu(self.bn1(self.conv1(inputs)))\n",
        "        x = self.leakyRelu(self.bn2(self.conv2(x)))\n",
        "        x = self.leakyRelu(self.bn3(self.conv3(x)))\n",
        "        x = self.leakyRelu(self.bn4(self.conv4(x)))\n",
        "        x = self.leakyRelu(self.bn5(self.conv5(x)))\n",
        "        x = self.conv6(x)\n",
        "        output = self.sig(self.flatten(x))\n",
        "        \n",
        "        return output"
      ],
      "id": "xfFPaaOVWRlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2SuKzuWRlu"
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        nc = 3\n",
        "        nz = 100\n",
        "        nf = 64\n",
        "        \n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels = nz, out_channels = nf*16, kernel_size = 4, stride = 1, padding = 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(nf*16)\n",
        "        self.lr1 = nn.LeakyReLU()\n",
        "        # (ngf*16) x 4 x 4\n",
        "\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=nf * 16,out_channels = nf * 8, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(nf * 8)\n",
        "        self.lr2 = nn.LeakyReLU()\n",
        "        # (ngf*8) x 8 x 8\n",
        "\n",
        "        self.conv3 = nn.ConvTranspose2d(in_channels = nf * 8, out_channels = nf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(nf * 4)\n",
        "        self.lr3 = nn.LeakyReLU()\n",
        "        # (ngf*4) x 16 x 16\n",
        "\n",
        "        self.conv4 = nn.ConvTranspose2d(in_channels = nf * 4, out_channels = nf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(nf * 2)\n",
        "        self.lr4 = nn.LeakyReLU()\n",
        "        # (ngf*2) x 32 x 32\n",
        "        \n",
        "        self.conv5 = nn.ConvTranspose2d(in_channels = nf * 2, out_channels = nf, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(nf)\n",
        "        self.lr5 = nn.LeakyReLU()\n",
        "        # (ngf) x 64 x 64\n",
        "\n",
        "        self.conv6 = nn.ConvTranspose2d(in_channels = nf, out_channels = nc, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "        # (nc) x 128 x 128\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.lr1(self.bn1(self.conv1(inputs)))\n",
        "        x = self.lr2(self.bn2(self.conv2(x)))\n",
        "        x = self.lr3(self.bn3(self.conv3(x)))\n",
        "        x = self.lr4(self.bn4(self.conv4(x)))\n",
        "        x = self.lr5(self.bn5(self.conv5(x)))\n",
        "        x = self.conv6(x)\n",
        "        output = self.tanh(x)\n",
        "        \n",
        "        return output"
      ],
      "id": "5V2SuKzuWRlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y91g0H-WhRNS"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "id": "y91g0H-WhRNS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "slCxdC9vWRlx"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "\n",
        "def train(gen, dis, train_dataloader, device, num_epochs=50):\n",
        "    \n",
        "    # optimizer_dis = optim.Adam(dis.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "    optimizer_gen = optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "    summary_path = \"./training_summary_\" + str(datetime.now())\n",
        "    summary_writer = torch.utils.tensorboard.SummaryWriter(summary_path)\n",
        "    \n",
        "    gen_losses = []\n",
        "    dis_losses = []\n",
        "\n",
        "    step = -1\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, batch_data in enumerate(train_dataloader):\n",
        "            step += 1\n",
        "            # dis.zero_grad()\n",
        "            x  = batch_data[0].to(device)\n",
        "\n",
        "            real_labels = torch.ones(len(x), requires_grad=False, device=device).to(device)\n",
        "            # discrimator_real = dis.forward(x.detach()).squeeze()\n",
        "            # error_real = nn.BCELoss()(discrimator_real, real_labels)\n",
        "            # error_real.backward(retain_graph=True)\n",
        "\n",
        "            # fake_labels = torch.zeros(len(x), device=device)\n",
        "            # discrimator_fake = dis.forward(fake_images).squeeze()\n",
        "            # error_fake = nn.BCELoss()(discrimator_fake, fake_labels)\n",
        "            # error_fake.backward(retain_graph=True)\n",
        "\n",
        "            # dis_error = error_real + error_fake\n",
        "            # dis_losses.append(dis_error.item())\n",
        "            # optimizer_dis.step()\n",
        "\n",
        "            \n",
        "\n",
        "            noise = torch.randn(len(x), 100, 1, 1).to(device)\n",
        "\n",
        "            fake_images = gen.forward(noise)\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            dis.eval()\n",
        "            output = dis.forward(fake_images).squeeze()\n",
        "\n",
        "            \n",
        "            gen_error = nn.BCELoss()(output, real_labels)\n",
        "            gen_error.backward()\n",
        "            gen_losses.append(gen_error.item())\n",
        "            optimizer_gen.step()\n",
        "            optimizer_gen.zero_grad()\n",
        "            accuracy = output.mean().item()\n",
        "\n",
        "            # if step % n_summary == 0:\n",
        "            summary_writer.add_scalar(\"Generator Loss\", gen_error, global_step=step)\n",
        "            # summary_writer.add_scalar(\"Discriminator Loss\", dis_error, global_step=step)\n",
        "            summary_writer.add_scalar(\"Discriminator Accuracy\", accuracy, global_step=step)\n",
        "\n",
        "\n",
        "\n",
        "            # if i % 100 == 0:\n",
        "            #   print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\t'\n",
        "            #         % (epoch, num_epochs, i, len(train_dataloader),\n",
        "            #           dis_error.item(), gen_error.item(), accuracy))\n",
        "\n",
        "            if i % 1 == 0:\n",
        "              print('[%d/%d][%d/%d]\\tLoss_G: %.4f\\tDiscriminator Output: %.4f\\t'\n",
        "                    % (epoch, num_epochs, i, len(train_dataloader),\n",
        "                      gen_error.item(), accuracy))\n",
        "              \n",
        "        if epoch % 2 == 0:\n",
        "          with torch.no_grad():\n",
        "            gen.eval()\n",
        "            zeroes = torch.randn(1, 100, 1, 1).to(device)\n",
        "            output = gen.forward(zeroes)\n",
        "            im = output.detach().cpu()\n",
        "\n",
        "            mean = np.asarray([ 0.5, 0.5, 0.5 ])\n",
        "            std = np.asarray([ 0.5, 0.5, 0.5 ]) \n",
        "\n",
        "            denormalize = transforms.Normalize((-1 * mean / std), (1.0 / std))\n",
        "            im = im.squeeze()\n",
        "\n",
        "            im = denormalize(im)\n",
        "\n",
        "            grid = torchvision.utils.make_grid(im)\n",
        "            summary_writer.add_image(f'images_epoch_{epoch}', grid, global_step=epoch)\n",
        "\n",
        "            im = np.transpose(im, (1, 2, 0))\n",
        "\n",
        "            im = im.numpy()\n",
        "            torch.save(gen.state_dict(), './drive/MyDrive/gan_images_frozen_gen/gendata.pt')\n",
        "            # torch.save(dis.state_dict(), './drive/MyDrive/gan_images_frozen_gen/disdata.pt')\n",
        "\n",
        "            # plt.imsave(f'./drive/MyDrive/gan_images/epoch{epoch}.png', im)\n",
        "\n",
        "          gen.train()\n",
        "\n",
        "              \n",
        "    summary_writer.close() \n",
        "    return(gen_losses, dis_losses)\n",
        "\n",
        "gen = Generator().to(device)\n",
        "# gen.apply(weights_init)\n",
        "gen.load_state_dict(torch.load('gendata.pt'))\n",
        "\n",
        "dis = Discriminator().to(device)\n",
        "dis.load_state_dict(torch.load('disdata.pt'))\n",
        "train(gen, dis, train_dataloader, device)\n"
      ],
      "id": "slCxdC9vWRlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj3KdxvOiUrV"
      },
      "source": [
        "!cp -r training_summary drive/MyDrive/gan_images/"
      ],
      "id": "pj3KdxvOiUrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQhssqw3zABT"
      },
      "source": [
        "gen = Generator().to(device)\n",
        "# gen.apply(weights_init)\n",
        "gen.load_state_dict(torch.load('gendata.pt'))\n",
        "\n",
        "oldgen = gen"
      ],
      "id": "JQhssqw3zABT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNzhWACoWRlz"
      },
      "source": [
        "torch.save(gen.state_dict(), './drive/MyDrive/gan_images/gendata.pt')\n",
        "torch.save(dis.state_dict(), './drive/MyDrive/gan_images/disdata.pt')\n",
        "gen.load_state_dict(torch.load('./drive/MyDrive/gan_images/gendata.pt'))\n"
      ],
      "id": "eNzhWACoWRlz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8lu2EZeWRly"
      },
      "source": [
        "rand = torch.randn(1, 100, 1, 1, device=device)\n",
        "\n",
        "output = gen.forward(rand)\n",
        "im = output.detach().cpu()\n",
        "im = img_denorm(im)\n",
        "# plt.imsave(f'./drive/MyDrive/gan_images/epoch.png', im)\n",
        "plt.imshow(im)\n",
        "\n"
      ],
      "id": "K8lu2EZeWRly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL5B3DlPac2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d80214-efaa-45e2-d5ab-ac1525b75b5a"
      },
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "gen = Generator().to(device)\n",
        "dis = Discriminator().to(device)\n",
        "summary(dis, (3,128,128))"
      ],
      "id": "mL5B3DlPac2v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           3,136\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "         LeakyReLU-3           [-1, 64, 64, 64]               0\n",
            "            Conv2d-4          [-1, 128, 32, 32]         131,200\n",
            "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
            "         LeakyReLU-6          [-1, 128, 32, 32]               0\n",
            "            Conv2d-7          [-1, 256, 16, 16]         524,544\n",
            "       BatchNorm2d-8          [-1, 256, 16, 16]             512\n",
            "         LeakyReLU-9          [-1, 256, 16, 16]               0\n",
            "           Conv2d-10            [-1, 512, 8, 8]       2,097,664\n",
            "      BatchNorm2d-11            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-12            [-1, 512, 8, 8]               0\n",
            "           Conv2d-13           [-1, 1024, 4, 4]       8,389,632\n",
            "      BatchNorm2d-14           [-1, 1024, 4, 4]           2,048\n",
            "        LeakyReLU-15           [-1, 1024, 4, 4]               0\n",
            "           Conv2d-16              [-1, 1, 1, 1]          16,385\n",
            "          Flatten-17                    [-1, 1]               0\n",
            "          Sigmoid-18                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 11,166,529\n",
            "Trainable params: 11,166,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 11.63\n",
            "Params size (MB): 42.60\n",
            "Estimated Total Size (MB): 54.41\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X878vwRUNQ7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dfeba8-b0b4-42fb-d5de-045d84cb205d"
      },
      "source": [
        "summary(gen, (100, 1, 1))"
      ],
      "id": "X878vwRUNQ7y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1           [-1, 1024, 4, 4]       1,638,400\n",
            "       BatchNorm2d-2           [-1, 1024, 4, 4]           2,048\n",
            "         LeakyReLU-3           [-1, 1024, 4, 4]               0\n",
            "   ConvTranspose2d-4            [-1, 512, 8, 8]       8,388,608\n",
            "       BatchNorm2d-5            [-1, 512, 8, 8]           1,024\n",
            "         LeakyReLU-6            [-1, 512, 8, 8]               0\n",
            "   ConvTranspose2d-7          [-1, 256, 16, 16]       2,097,152\n",
            "       BatchNorm2d-8          [-1, 256, 16, 16]             512\n",
            "         LeakyReLU-9          [-1, 256, 16, 16]               0\n",
            "  ConvTranspose2d-10          [-1, 128, 32, 32]         524,288\n",
            "      BatchNorm2d-11          [-1, 128, 32, 32]             256\n",
            "        LeakyReLU-12          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-13           [-1, 64, 64, 64]         131,072\n",
            "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
            "        LeakyReLU-15           [-1, 64, 64, 64]               0\n",
            "  ConvTranspose2d-16          [-1, 3, 128, 128]           3,072\n",
            "             Tanh-17          [-1, 3, 128, 128]               0\n",
            "================================================================\n",
            "Total params: 12,786,560\n",
            "Trainable params: 12,786,560\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 12.38\n",
            "Params size (MB): 48.78\n",
            "Estimated Total Size (MB): 61.15\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaLL0wLfv7H0"
      },
      "source": [
        ""
      ],
      "id": "BaLL0wLfv7H0",
      "execution_count": null,
      "outputs": []
    }
  ]
}